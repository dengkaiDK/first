import os
import tensorflow as tf
import numpy as np
import sys
import tarfile
import matplotlib.pyplot as plt
sess=tf.Session()

os.chdir('C:\\Users\\dengkai\\working_space')
os.getcwd()

data_dir='C:\\Users\\dengkai\\working_space\\image\\light\\train'
data_dir2='C:\\Users\\dengkai\\working_space\\image\\light\\test'
data_dir3='C:\\Users\\dengkai\\working_space\\image\\ult\\train'
data_dir4='C:\\Users\\dengkai\\working_space\\image\\ult\\test'

def get_list(data_dir):
  block=[]
  block_label=[]
  empty=[]
  empty_label=[]
  for file in os.listdir(data_dir):
    if(file.split('_')[0]=='block'):
      block.append(os.path.join(data_dir,file))
      block_label.append(1)
    if(file.split('_')[0]=='empty'):
      empty.append(os.path.join(data_dir,file))
      empty_label.append(0)
  print('there are {} block and {} empty'.format(len(block),len(empty)))
  image=np.hstack([block,empty])
  label=np.hstack([block_label,empty_label])
  expand=np.hstack([np.zeros(len(block_label)),np.ones(len(empty_label))])
  label_image=np.vstack((image,label,expand))
  label_image=label_image.transpose()
  return label_image
image_light_train=get_list(data_dir)
image_light_test=get_list(data_dir2)
image_ult_train=get_list(data_dir3)
image_ult_test=get_list(data_dir4)
#超参数
batch_size=100
crop_width=32
crop_height=32
model_learning_rate=0.1
num_targets=2
output_every=50
eval_every=500
generations=2000
image_height=40
image_width=40
num_channels=3
image_vec_length=image_height*image_width*num_channels

def read_cifar_files(filename_queue,distort_images=True):
  image_reader = tf.WholeFileReader()  
  key, image = image_reader.read(file_queue)
  image = tf.image.decode_jpeg(image)
  reshaped_image=tf.cast(image,tf.float32)
  final_image=tf.image.resize_image_with_crop_or_pad(reshaped_image,crop_width,crop_height)
  '''
  if distort_images:
    final_image=tf.image.random_brightness(final_image,max_delta=63)
    final_image=tf.image.random_flip_left_right(final_image)
    final_image=tf.image.random_contrast(final_image,lower=0.2,upper=1.8)
  '''
  return (final_image)
  
def input_pipeline(files,label,batch_size):
  file_queue = tf.train.string_input_producer(files, shuffle=False, num_epochs=len(files))#通过转化成队列，进入图像管道填充函数
  image=read_cifar_files(file_queue)#通过图像管道填充函数将路径队列转化为图像和标签
  label=tf.cast(label,tf.float32)
  min_after_dequeue=1000
  capacity=min_after_dequeue+3*batch_size
  example_batch,label_batch=tf.train.shuffle_batch([image,label],batch_size,capacity,min_after_dequeue)#随机抽取样本
  return (image_batch,label_batch)
def cifar_cnn_model(input_images,batch_size,train_logical=True):#卷积神经网络模型
  def truncated_normal_var(name,shape,dtype):
    return (tf.get_variable(name=name,shape=shape,dtype=dtype,initializer=tf.truncated_normal_initializer(stddev=0.05)))
  def zero_var(name,shape,dtype):
    return (tf.get_variable(name=name,shape=shape,dtype=dtype,initializer=tf.constant_initializer(0.0)))
  with tf.variable_scope('conv1') as scope:
    conv1_kernel=truncated_normal_var(name='conv_kernel1',shape=[5,5,3,64],dtype=tf.float32)
    conv1=tf.nn.conv2d(input_images,conv1_kernel,[1,1,1,1],padding='SAME')
    conv1_bias=zero_var(name='conv1_bias',shape=[64],dtype=tf.float32)
    conv1_add_bias=tf.nn.bias_add(conv1,conv1_bias)
    relu_conv1=tf.nn.relu(conv1_add_bias)
  pool1=tf.nn.max_pool(relu_conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME',name='pool_layer1')
  norm1=tf.nn.lrn(pool1,depth_radius=5,bias=2.0,alpha=1e-3,beta=0.75,name='norm1')
  with tf.variable_scope('conv2') as scope:
    conv2_kernel=truncated_normal_var(name='conv_kernel2',shape=[5,5,64,64],dtype=tf.float32)
    conv2=tf.nn.conv2d(norm1,conv2_kernel,[1,1,1,1],padding='SAME')
    conv2_bias=zero_var(name='conv_bias2',shape=[64],dtype=tf.float32)
    conv2_add_bias=tf.nn.bias_add(conv2,conv2_bias)
    relu_conv2=tf.nn.relu(conv2_add_bias)
  pool2=tf.nn.max_pool(relu_conv2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME',name='pool_layer2')
  norm2=tf.nn.lrn(pool2,depth_radius=5,bias=2.0,alpha=1e-3,beta=0.75,name='norm2')
  reshaped_output=tf.reshape(norm2,[batch_size,-1])
  reshaped_dim=reshaped_output.get_shape()[1].value
  
  with tf.variable_scope('full1') as scope:
    full_weight1=truncated_normal_var(name='full_mult1',shape=[reshaped_dim,384],dtype=tf.float32)
    full_bias1=zero_var(name='full_bias1',shape=[384],dtype=tf.float32)
    full_layer1=tf.nn.relu(tf.add(tf.matmul(reshaped_output,full_weight1),full_bias1))
  with tf.variable_scope('full2') as scope:
    full_weight2=truncated_normal_var(name='full_mult2',shape=[384,192],dtype=tf.float32)
    full_bias2=zero_var(name='full_bias2',shape=[192],dtype=tf.float32)
    full_layer2=tf.nn.relu(tf.add(tf.matmul(full_layer1,full_weight2),full_bias2))
  with tf.variable_scope('full3') as scope:
    full_weight3=truncated_normal_var(name='full_mult3',shape=[192,num_targets],dtype=tf.float32)
    full_bias3=zero_var(name='full_bias3',shape=[num_targets],dtype=tf.float32)
    final_output=tf.add(tf.matmul(full_layer2,full_weight3),full_bias3)
  return final_output
def cifar_loss(logits,targets):#交叉熵损失函数
  targets=tf.squeeze(tf.cast(targets,tf.int32))
  cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=targets)
  cross_entropy_mean=tf.reduce_mean(cross_entropy)
  return cross_entropy_mean
def train_step(loss_value):#学习率逐渐降低
  #model_learning_rate=tf.train.exponential_decay(learning_rate,generation_num,num_gens_to_wait,lr_decay,staircase=True)
  my_optimizer=tf.train.GradientDescentOptimizer(model_learning_rate)
  train_step=my_optimizer.minimize(loss_value)
  return train_step
def accuracy_of_batch(logits,targets):#准确率预测函数
  targets=tf.squeeze(tf.cast(targets,tf.int32))
  batch_prediction=tf.cast(tf.argmax(logits,1),tf.int32)
  predicted_correctly=tf.equal(batch_prediction,targets)
  accuracy=tf.reduce_mean(tf.cast(predicted_correctly,tf.float32))
  return accuracy
images,targets=input_pipeline(image_light_train[:,0],image_light_train[:,1:],batch_size)
test_images,test_targets=input_pipeline(image_light_test[:,0],image_light_test[:,1:],batch_size)
with tf.variable_scope('model_definition') as scope:
  model_output=cifar_cnn_model(images,batch_size)
  scope.reuse_variables()
  test_output=cifar_cnn_model(test_images,batch_size)
loss=cifar_loss(model_output,targets)
accuracy=accuracy_of_batch(test_output,test_targets)
train_op=train_step(loss)

init=tf.initialize_all_variables()
sess.run(init)
tf.train.start_queue_runners(sess=sess)

train_loss=[]
test_accuracy=[]
for i in range(generations):
  _,loss_value=sess.run([train_op,loss])
  if (i+1)%output_every==0:
    train_loss.append(loss_value)
    output='Generation{}: Loss={:.5f}'.format((i+1),loss_value)
    print(output)
  if (i+1)%eval_every==0:
    [temp_accuracy]=sess.run([accuracy])
    test_accuracy.append(temp_accuracy)
    acc_output='---Test accuracy={:.2f}%.'.format(100.*temp_accuracy)
    print(acc_output)
eval_indices=range(0,generations,eval_every)
output_indices=range(0,generetions,output_every)
#可视化
plt.figure()
plt.plot(output_indices,train_loss,'k-')
plt.title('Softmax loss per generation')
plt.xlabel('Genaration')
plt.ylabel('Softmax Loss')
plt.show()
plt.figure()
plt.plot(eval_indices,test_accuracy,'r-')
plt.title('Test Accuracy')
plt.xlabel('Generation')
plt.ylabel('Accuracy')
plt.show()
